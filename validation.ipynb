{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from numerapi import NumerAPI\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define paths\n",
    "model_path = \"saved_models/model.pth\"  # Replace with your model file name\n",
    "features_file_path = \"saved_models/features.txt\"  # File containing the selected features\n",
    "validation_data_path = \"data/validation.parquet\"\n",
    "features_json_path = \"data/features.json\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr):\n",
    "        super(ClassificationLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.class_to_bucket = {0: 0, 1: 0.25, 2: 0.5, 3: 0.75, 4: 1}\n",
    "\n",
    "        self.history = {'loss': [], 'f1_score': []}\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, _ = self.lstm(x)\n",
    "        x = self.linear(h[:, -1, :])  # Take last output for classification\n",
    "        return x\n",
    "\n",
    "    def train_model(self, train_loader, num_epochs, device):\n",
    "        self.to(device)\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            all_preds, all_labels = [], []\n",
    "            for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                label_indices = torch.tensor([self.bucket_to_class(val) for val in labels.cpu().numpy()], dtype=torch.long).to(device)\n",
    "                loss = self.criterion(outputs, label_indices)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(label_indices.cpu().numpy())\n",
    "\n",
    "            avg_loss = epoch_loss / len(train_loader)\n",
    "            f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, F1 Score: {f1:.4f}\")\n",
    "            self.history['loss'].append(avg_loss)\n",
    "            self.history['f1_score'].append(f1)\n",
    "\n",
    "    def bucket_to_class(self, val):\n",
    "        bucket_to_class = {v: k for k, v in self.class_to_bucket.items()}\n",
    "        return bucket_to_class[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationLSTM(\n",
       "  (lstm): LSTM(40, 128, batch_first=True)\n",
       "  (linear): Linear(in_features=128, out_features=5, bias=True)\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(model_path, map_location=device)\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features for validation: ['feature_undiscoverable_tickling_volume', 'feature_haunched_cognominal_eyesore', 'feature_syndicalist_crackle_tragacanth', 'feature_organometallic_mercantile_baton', 'feature_analgesic_pensionary_exterior', 'feature_unshadowed_biometric_chokebore', 'feature_adminicular_shod_levant', 'feature_substandard_practicable_slobber', 'feature_shriveled_blightingly_laud', 'feature_catechetical_paragogical_accouterment', 'feature_vestigial_tittering_cyan', 'feature_drawn_gimcrack_vulcanalia', 'feature_haemostatic_pulpiest_pembroke', 'feature_egotistical_carotid_irrationality', 'feature_preachy_uncontaminated_servitude', 'feature_cosier_aerial_yoga', 'feature_unmovable_declassified_corrival', 'feature_upbeat_boneheaded_chequer', 'feature_simulated_shakiest_divisibility', 'feature_unfit_threatful_strontium', 'feature_xanthochroid_petrified_gutenberg', 'feature_debonnaire_opulent_stayer', 'feature_nonpersistent_miffiest_contemplator', 'feature_interlaminar_starlike_durbar', 'feature_inalterable_psilanthropic_rhotacism', 'feature_gnotobiotic_brittle_poultice', 'feature_verticillated_tenured_bosch', 'feature_choreic_sterilized_lagune', 'feature_interunion_tectricial_diaphone', 'feature_riparian_genteel_insalubrity', 'feature_rejective_carinate_ally', 'feature_whapping_liny_prelate', 'feature_chattier_tight_academic', 'feature_mullioned_hidden_niece', 'feature_muscly_splintery_stope', 'feature_appraising_chasmogamic_picrate', 'feature_homier_congestive_queening', 'feature_dorsal_phenological_hodograph', 'feature_trapped_antipapal_buffer', 'feature_calendrical_dextral_tantrum']\n"
     ]
    }
   ],
   "source": [
    "with open(features_file_path, \"r\") as f:\n",
    "    selected_features = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"Selected features for validation: {selected_features}\")\n",
    "\n",
    "# Load features metadata for reference\n",
    "with open(features_json_path, \"r\") as f:\n",
    "    feature_metadata = json.load(f)\n",
    "target_cols = feature_metadata[\"targets\"]\n",
    "\n",
    "# Load and filter the validation data to include only selected features\n",
    "validation_data = pd.read_parquet(\n",
    "    validation_data_path,\n",
    "    columns=[\"era\", \"data_type\"] + selected_features + target_cols\n",
    ")\n",
    "\n",
    "# Filter for validation rows only\n",
    "validation_data = validation_data[validation_data[\"data_type\"] == \"validation\"]\n",
    "validation_data = validation_data.drop(columns=[\"data_type\"])\n",
    "validation = validation_data[['era','target'] + selected_features]\n",
    "validation = validation[validation_data[\"era\"].isin(validation[\"era\"].unique()[::5])]\n",
    "\n",
    "# Remove embargo eras\n",
    "last_train_era = int(validation[\"era\"].unique()[-1])\n",
    "eras_to_embargo = [str(era).zfill(4) for era in [last_train_era + i for i in range(4)]]\n",
    "validation = validation[~validation[\"era\"].isin(eras_to_embargo)]\n",
    "validation.sort_values(by=[\"era\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feature = selected_features\n",
    "X_val = validation[final_feature]\n",
    "y_val = validation[\"target\"]\n",
    "\n",
    "# Define class mapping\n",
    "bucket_to_class = {0: 0, 0.25: 1, 0.5: 2, 0.75: 3, 1: 4}\n",
    "y_val_class_indices = y_val.map(bucket_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_len = 5\n",
    "\n",
    "Xtest = validation.iloc[:,2:]\n",
    "ytest = validation.iloc[:,1]\n",
    "\n",
    "def preprocess_validation_data(validation_df, window_len, features):\n",
    "    Xraw = validation_df[features]\n",
    "    Xraw_filled = Xraw.fillna(-1)\n",
    "\n",
    "    new_data = []\n",
    "\n",
    "    # Pad the start of the data with the same number of missing values as window_len - 1\n",
    "    padding = torch.full((window_len - 1, len(features)), -1)  # -1 for padding\n",
    "    Xraw_padded = torch.cat((padding, torch.tensor(Xraw_filled.values, dtype=torch.float32)))\n",
    "\n",
    "    for start in range(0, len(Xraw_filled)):\n",
    "        new_row_data = Xraw_padded[start : start + window_len].reshape(window_len, len(features))\n",
    "        new_data.append(new_row_data)\n",
    "\n",
    "    return torch.stack(new_data)\n",
    "\n",
    "\n",
    "# X_val = preprocess_validation_data(validation_df,window_len,features)\n",
    "X_val = preprocess_validation_data(validation,window_len,final_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(X_val)\n",
    "    predictions = torch.argmax(outputs, dim=1)  \n",
    "    predictions = predictions.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall F1 Score: 0.1914\n",
      "Overall Accuracy: 0.4419\n",
      "\n",
      "Classwise Metrics:\n",
      "Class 0:\n",
      "  F1 Score: 0.0413\n",
      "  Accuracy: 0.0021\n",
      "Class 1:\n",
      "  F1 Score: 0.1146\n",
      "  Accuracy: 0.0164\n",
      "Class 2:\n",
      "  F1 Score: 0.6252\n",
      "  Accuracy: 0.4119\n",
      "Class 3:\n",
      "  F1 Score: 0.1319\n",
      "  Accuracy: 0.0200\n",
      "Class 4:\n",
      "  F1 Score: 0.0438\n",
      "  Accuracy: 0.0024\n"
     ]
    }
   ],
   "source": [
    "# Convert predictions back to original bucket values if needed\n",
    "class_to_bucket = {v: k for k, v in bucket_to_class.items()}\n",
    "predicted_buckets = [class_to_bucket[pred] for pred in predictions]\n",
    "\n",
    "# Calculate Overall F1 Score and Accuracy\n",
    "overall_f1 = f1_score(y_val_class_indices, predictions, average=\"macro\")\n",
    "overall_accuracy = accuracy_score(y_val_class_indices, predictions)\n",
    "print(f\"Overall F1 Score: {overall_f1:.4f}\")\n",
    "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "\n",
    "# Classwise Metrics\n",
    "classwise_report = classification_report(y_val_class_indices, predictions, output_dict=True)\n",
    "print(\"\\nClasswise Metrics:\")\n",
    "for class_label, metrics in classwise_report.items():\n",
    "    if class_label not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "        print(f\"Class {class_label}:\")\n",
    "        print(f\"  F1 Score: {metrics['f1-score']:.4f}\")\n",
    "        print(f\"  Accuracy: {metrics['precision'] * metrics['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
